#!/usr/bin/env python3
"""
Optimized Malicious URL Detection Pipeline
Scrapes URL content in a minimal Docker container, transfers to ADK agent for analysis
"""

import os
import json
import uuid
import time
import docker
import logging
from pathlib import Path
from typing import Dict, Any, Optional
import requests
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# CONFIGURATION
# ============================================================================

DOCKER_CLIENT = docker.from_env()
SHARED_VOLUME_NAME = "malware_detection_volume"
SCRAPER_MOUNT_PATH = "/data"
ANALYZER_MOUNT_PATH = "/data"
SCRAPER_IMAGE = "malware-scraper:latest"
ANALYZER_IMAGE = "malware-analyzer:latest"
DOCKER_NETWORK_NAME = "malware_detection_net"
REQUEST_TIMEOUT = 30
MAX_RETRIES = 3

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def setup_docker_resources() -> None:
    """Create shared Docker network and volume if not exist."""
    try:
        DOCKER_CLIENT.networks.get(DOCKER_NETWORK_NAME)
        logger.info(f"Network '{DOCKER_NETWORK_NAME}' already exists")
    except docker.errors.NotFound:
        DOCKER_CLIENT.networks.create(
            DOCKER_NETWORK_NAME,
            driver="bridge",
            check_duplicate=True
        )
        logger.info(f"Created network: {DOCKER_NETWORK_NAME}")

    try:
        DOCKER_CLIENT.volumes.get(SHARED_VOLUME_NAME)
        logger.info(f"Volume '{SHARED_VOLUME_NAME}' already exists")
    except docker.errors.NotFound:
        DOCKER_CLIENT.volumes.create(SHARED_VOLUME_NAME)
        logger.info(f"Created volume: {SHARED_VOLUME_NAME}")

def generate_task_id() -> str:
    """Generate unique task identifier."""
    return f"task_{uuid.uuid4().hex[:8]}_{int(time.time())}"

def cleanup_docker_resources() -> None:
    """Clean up Docker network and volume (optional)."""
    try:
        network = DOCKER_CLIENT.networks.get(DOCKER_NETWORK_NAME)
        network.remove()
        logger.info(f"Removed network: {DOCKER_NETWORK_NAME}")
    except Exception as e:
        logger.warning(f"Could not remove network: {e}")

def wait_for_container_readiness(container_name: str, port: int, timeout: int = 30) -> bool:
    """Wait for container to be ready by checking HTTP health endpoint."""
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            response = requests.get(
                f"http://localhost:{port}/health",
                timeout=2
            )
            if response.status_code == 200:
                logger.info(f"Container {container_name} is ready")
                return True
        except (requests.ConnectionError, requests.Timeout):
            time.sleep(1)
    logger.error(f"Container {container_name} failed to become ready")
    return False

# ============================================================================
# SCRAPER CONTAINER MANAGEMENT
# ============================================================================

def run_scraper_container(url: str, task_id: str) -> Optional[str]:
    """
    Run scraper in minimal container, extract and store data.
    Returns container ID or None on failure.
    """
    container_name = f"scraper_{task_id}"
    
    try:
        # Create request file for scraper
        request_file = {
            "url": url,
            "task_id": task_id,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        logger.info(f"Starting scraper container: {container_name}")
        
        container = DOCKER_CLIENT.containers.run(
            SCRAPER_IMAGE,
            name=container_name,
            volumes={
                SHARED_VOLUME_NAME: {"bind": SCRAPER_MOUNT_PATH, "mode": "rw"}
            },
            network=DOCKER_NETWORK_NAME,
            environment={
                "TARGET_URL": url,
                "TASK_ID": task_id,
                "OUTPUT_PATH": SCRAPER_MOUNT_PATH
            },
            detach=True,
            remove=False,
            timeout=REQUEST_TIMEOUT
        )
        
        logger.info(f"Scraper container started: {container.id[:12]}")
        
        # Wait for completion
        exit_code = container.wait(timeout=REQUEST_TIMEOUT)
        
        if exit_code["StatusCode"] == 0:
            # Get container logs
            logs = container.logs(stdout=True, stderr=True).decode('utf-8')
            logger.info(f"Scraper logs: {logs}")
            logger.info(f"Scraper completed successfully for {url}")
            return container.id
        else:
            logger.error(f"Scraper failed with exit code: {exit_code['StatusCode']}")
            logger.error(f"Error logs: {container.logs(stderr=True).decode('utf-8')}")
            return None
            
    except docker.errors.ImageNotFound:
        logger.error(f"Docker image not found: {SCRAPER_IMAGE}")
        return None
    except docker.errors.ContainerError as e:
        logger.error(f"Container error: {e}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error in scraper: {e}")
        return None
    finally:
        # Cleanup scraper container
        try:
            container = DOCKER_CLIENT.containers.get(container_name)
            container.stop(timeout=5)
            container.remove()
            logger.info(f"Cleaned up scraper container: {container_name}")
        except Exception as e:
            logger.warning(f"Error cleaning up scraper: {e}")

# ============================================================================
# ANALYZER CONTAINER MANAGEMENT
# ============================================================================

def run_analyzer_container(task_id: str) -> Optional[Dict[str, Any]]:
    """
    Run ADK analyzer in container to detect malicious content.
    Returns analysis result or None on failure.
    """
    container_name = f"analyzer_{task_id}"
    result = None
    
    try:
        logger.info(f"Starting analyzer container: {container_name}")
        
        container = DOCKER_CLIENT.containers.run(
            ANALYZER_IMAGE,
            name=container_name,
            volumes={
                SHARED_VOLUME_NAME: {"bind": ANALYZER_MOUNT_PATH, "mode": "ro"}
            },
            network=DOCKER_NETWORK_NAME,
            environment={
                "TASK_ID": task_id,
                "INPUT_PATH": ANALYZER_MOUNT_PATH,
                "GOOGLE_API_KEY": os.getenv("GOOGLE_API_KEY", "")
            },
            detach=True,
            remove=False,
            timeout=REQUEST_TIMEOUT * 2
        )
        
        logger.info(f"Analyzer container started: {container.id[:12]}")
        
        # Wait for completion
        exit_code = container.wait(timeout=REQUEST_TIMEOUT * 2)
        
        if exit_code["StatusCode"] == 0:
            logs = container.logs(stdout=True, stderr=True).decode('utf-8')
            logger.info(f"Analyzer logs: {logs}")
            
            # Try to read result from volume
            result = read_analysis_result(task_id)
            logger.info(f"Analysis completed for task {task_id}")
        else:
            logger.error(f"Analyzer failed with exit code: {exit_code['StatusCode']}")
            logger.error(f"Error logs: {container.logs(stderr=True).decode('utf-8')}")
            
    except docker.errors.ImageNotFound:
        logger.error(f"Docker image not found: {ANALYZER_IMAGE}")
    except Exception as e:
        logger.error(f"Unexpected error in analyzer: {e}")
    finally:
        # Cleanup analyzer container
        try:
            container = DOCKER_CLIENT.containers.get(container_name)
            container.stop(timeout=5)
            container.remove()
            logger.info(f"Cleaned up analyzer container: {container_name}")
        except Exception as e:
            logger.warning(f"Error cleaning up analyzer: {e}")
    
    return result

def read_analysis_result(task_id: str) -> Optional[Dict[str, Any]]:
    """Read analysis result from shared volume."""
    try:
        # This would typically be in a mounted volume
        # For demonstration, we'll return a structured result
        logger.info(f"Retrieving analysis result for task {task_id}")
        
        # In production, read from the shared volume filesystem
        # result_path = f"/mnt/docker/{SHARED_VOLUME_NAME}/result_{task_id}.json"
        # if os.path.exists(result_path):
        #     with open(result_path, 'r') as f:
        #         return json.load(f)
        
        return {
            "task_id": task_id,
            "status": "completed",
            "is_malicious": False,
            "confidence": 0.95,
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error(f"Error reading analysis result: {e}")
        return None

# ============================================================================
# MAIN ORCHESTRATION
# ============================================================================

def analyze_url(url: str) -> Dict[str, Any]:
    """
    Main function: orchestrate entire malware detection pipeline.
    
    Process:
    1. Create task ID
    2. Run scraper in minimal container
    3. Transfer data via shared volume
    4. Run analyzer with ADK agent
    5. Return results
    6. Cleanup resources
    """
    task_id = generate_task_id()
    logger.info(f"Starting malware detection for URL: {url}")
    logger.info(f"Task ID: {task_id}")
    
    # Setup Docker resources
    setup_docker_resources()
    
    result = {
        "task_id": task_id,
        "url": url,
        "status": "failed",
        "error": None,
        "analysis": None,
        "timestamp": datetime.utcnow().isoformat()
    }
    
    try:
        # Step 1: Run scraper
        logger.info("Step 1: Running scraper...")
        scraper_container_id = run_scraper_container(url, task_id)
        
        if not scraper_container_id:
            result["error"] = "Scraper container failed"
            logger.error("Scraper failed, aborting analysis")
            return result
        
        # Step 2: Wait for data transfer
        logger.info("Step 2: Waiting for data transfer...")
        time.sleep(2)  # Allow time for data to be written
        
        # Step 3: Run analyzer
        logger.info("Step 3: Running analyzer...")
        analysis_result = run_analyzer_container(task_id)
        
        if analysis_result:
            result["status"] = "success"
            result["analysis"] = analysis_result
            logger.info(f"Analysis result: {analysis_result}")
        else:
            result["error"] = "Analyzer container failed"
            logger.error("Analyzer failed")
            
    except Exception as e:
        logger.error(f"Pipeline error: {e}")
        result["error"] = str(e)
    
    return result

def analyze_multiple_urls(urls: list) -> list:
    """Analyze multiple URLs in sequence."""
    results = []
    for url in urls:
        try:
            result = analyze_url(url)
            results.append(result)
            logger.info(f"Completed analysis for {url}")
        except Exception as e:
            logger.error(f"Failed to analyze {url}: {e}")
            results.append({
                "url": url,
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            })
    
    return results

# ============================================================================
# ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    # Example usage
    test_urls = [
        "https://example.com",
        "https://github.com",
    ]
    
    logger.info("Starting Malware Detection Pipeline")
    logger.info(f"Analyzing {len(test_urls)} URLs")
    
    results = analyze_multiple_urls(test_urls)
    
    logger.info("\n" + "="*80)
    logger.info("FINAL RESULTS")
    logger.info("="*80)
    
    for result in results:
        logger.info(json.dumps(result, indent=2))
    
    # Optional: cleanup resources
    # cleanup_docker_resources()
    
    logger.info("Pipeline completed")
